{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_4_Regresion_Log%C3%ADstica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKoHCRrL5J7e"
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmT3pvr67REf"
   },
   "source": [
    "Datos: ILPD (Indian Liver Patient Dataset) Data Set\n",
    "* [Descarga haciendo click en este enlace](https://github.com/txusser/Master_IA_Sanidad/blob/main/Modulo_2/datos/Indian_Liver_Patient_Dataset_(ILPD).csv)\n",
    "\n",
    "The Indian Liver Patient Dataset (ILPD) is a dataset used for the analysis and prediction of liver diseases in patients. This dataset comes from a database of Indian patients and aims to assist in identifying individuals who may be suffering from liver diseases based on various clinical and biochemical characteristics.\n",
    "\n",
    "### Dataset Characteristics:\n",
    "- **Number of instances**: 583 patients\n",
    "- **Number of variables/features**: 10+1\n",
    "\n",
    "### Variables in the ILPD dataset:\n",
    "\n",
    "1. **Age**: Patient's age\n",
    "   - **Description**: Indicates the age of the patient in years.\n",
    "\n",
    "2. **Gender**: Patient's gender\n",
    "   - **Description**: Indicates the gender of the patient, where 'Male' corresponds to male and 'Female' to female.\n",
    "\n",
    "3. **Total_Bilirubin (TB)**: Total bilirubin\n",
    "   - **Description**: The total amount of bilirubin in the blood. Bilirubin is a yellow substance produced during the normal breakdown of red blood cells. High levels can indicate liver problems.\n",
    "\n",
    "4. **Direct_Bilirubin (DB)**: Direct bilirubin\n",
    "   - **Description**: The amount of conjugated bilirubin in the blood. Direct bilirubin is bound to other molecules that make it water-soluble and can be a more specific indicator of liver diseases.\n",
    "\n",
    "5. **Alkaline_Phosphotase (Alkphos)**: Alkaline phosphatase\n",
    "   - **Description**: An enzyme related to the bile duct. Elevated levels may indicate bile duct blockage or liver disease.\n",
    "\n",
    "6. **Alamine_Aminotransferase (Sgpt)**: Alanine aminotransferase\n",
    "   - **Description**: An enzyme primarily found in the liver. Elevated levels can be a sign of liver damage.\n",
    "\n",
    "7. **Aspartate_Aminotransferase (Sgot)**: Aspartate aminotransferase\n",
    "   - **Description**: An enzyme found in the liver and other tissues of the body. Elevated levels may indicate liver damage.\n",
    "\n",
    "8. **Total_Proteins (TP)**: Total proteins\n",
    "   - **Description**: The total amount of proteins in the blood. Proteins are essential for the structure and function of all cells in the body.\n",
    "\n",
    "9. **Albumin (ALB)**: Albumin\n",
    "   - **Description**: A protein produced by the liver that helps maintain blood volume and pressure. Low levels may indicate liver problems.\n",
    "\n",
    "10. **Albumin_and_Globulin_Ratio (A/G Ratio)**: Albumin-to-globulin ratio\n",
    "    - **Description**: The proportion of albumin to globulin in the blood. This ratio can help identify different types of liver diseases.\n",
    "\n",
    "11. **Dataset (Selector)**: Data selector\n",
    "    - **Description**: A field used to split the data into two sets (labeled by experts). It is generally used to indicate whether the patient has liver disease (1) or not (2).\n",
    "\n",
    "This explanation aims to provide a better understanding of the content and purpose of each column in the ILPD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrD8MSXp_k8_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables\n",
    "from sklearn.preprocessing import StandardScaler  # For applying value transformations\n",
    "from statsmodels.tools import add_constant  # To add a column of ones (constant) to the dataset\n",
    "import statsmodels.api as sm  # To build a logistic regression model for feature selection\n",
    "from scipy import stats  # To perform statistical calculations\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression model to fit the data\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score  # Model evaluation metrics\n",
    "import matplotlib.pyplot as plt  # Library for creating plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8jR_5N1-DgS"
   },
   "source": [
    "## 1. Review and processing of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es5vx5D0JPPI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/Indian_Liver_Patient_Dataset_(ILPD).csv\")\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Check the dataset status to determine if any imputation operations are necessary\n",
    "\n",
    "# Identify missing values\n",
    "missing_value_count = df.isnull().sum()\n",
    "missing_value_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Create a DataFrame to display the count and percentage of missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Count': missing_value_count,\n",
    "    'Percentage': missing_value_percentage\n",
    "})\n",
    "\n",
    "print(f\" - Missing values: \\n {missing_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOaAmFVC8hO3"
   },
   "outputs": [],
   "source": [
    "# Imputation of missing values using the SimpleImputer algorithm from scikit-learn\n",
    "# We fill missing values based on the median of the column to be imputed. Note: this\n",
    "# procedure (median imputation) is applied to numerical data columns.\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Missing values after the operation\n",
    "print(f\" => Number of missing values: \\n {df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Wl1OGc-Iht"
   },
   "source": [
    "## 2. Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF6cDWV198OL"
   },
   "outputs": [],
   "source": [
    "# Data Transformation Operations\n",
    "\n",
    "# Transform the 'SEXO' column into numerical variables using Label Encoding\n",
    "print(\"Values in the 'SEXO' column:\", np.unique(df['SEXO']))\n",
    "\n",
    "# Use a dictionary to perform the encoding, mapping Male to 1 and Female to 0\n",
    "df['SEXO'] = df['SEXO'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# Remap the target variable to take binary values 1 or 0\n",
    "df['CLASS'] = df['CLASS'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Verify the data types and the transformations performed\n",
    "print(\"\\n - Data types:\", df.dtypes)\n",
    "print(f\"\\n - First rows of the dataset after categorical variable encoding:\\n {df.head(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YpsY1e_CnF5"
   },
   "source": [
    "## 3. Feature Selection\n",
    "We will explore how to use a logistic regression model to identify the most relevant features by statistically evaluating their predictive capacity. Feature selection is a crucial step in building machine learning models because it improves model interpretability, reduces overfitting, and enhances overall model performance.\n",
    "\n",
    "### Feature Selection Process Context:\n",
    "\n",
    "1. **Add a Constant**:\n",
    "   - **Purpose**: Include an intercept in the logistic regression model. This allows the model to correctly adjust the baseline prediction.\n",
    "\n",
    "2. **Fit the Logit Model**:\n",
    "   - **Purpose**: Train the logistic regression model using the dataset features. This involves finding the coefficients that best relate the independent features to the dependent variable.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - **Purpose**: Assess the model's fit through the summary of the model adjustment. The summary includes metrics such as coefficients, standard errors, p-values, etc.\n",
    "\n",
    "4. **Feature Selection Based on p-values**:\n",
    "   - **Purpose**: Identify which features are statistically significant in predicting the target variable. P-values measure the probability that the observed coefficients differ from zero due to chance.\n",
    "   - **Process**: Relevant features are those with p-values less than a threshold (typically 0.05), indicating there is less than a 5% chance that the observed association is due to random variation.\n",
    "\n",
    "### Importance of the Process:\n",
    "\n",
    "- **Dimensionality Reduction**: By identifying and selecting only the most relevant features, the number of variables in the model can be reduced, simplifying the model and potentially improving its performance.\n",
    "- **Model Performance Improvement**: Removing irrelevant or redundant features can enhance model accuracy and reduce overfitting risk.\n",
    "- **Interpretability**: A model with fewer features is easier to interpret and understand, which is crucial in applications such as medicine or finance.\n",
    "- **Computational Efficiency**: Simpler models require fewer computational resources to train and evaluate.\n",
    "\n",
    "In summary, the feature selection process using logistic regression models and p-values is a powerful technique for building effective and efficient predictive models, allowing analysts to focus on the variables that truly matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbbfSp9NxhoU"
   },
   "outputs": [],
   "source": [
    "# Add a constant to the set of features\n",
    "X = df.drop('CLASS', axis=1)\n",
    "\n",
    "df_constant = sm.add_constant(X)\n",
    "\n",
    "# Fit the Logit model\n",
    "model = sm.Logit(df['CLASS'], df_constant)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(f\" - Results: {result.summary()}\")\n",
    "\n",
    "# Identify the most relevant features\n",
    "\n",
    "p_values = result.pvalues\n",
    "relevant_features = p_values[p_values < 0.05].index.tolist()\n",
    "\n",
    "# Remove the constant from relevant features if present\n",
    "if 'const' in relevant_features:\n",
    "    relevant_features.remove('const')\n",
    "\n",
    "print(\"Relevant features based on p-values:\", relevant_features)\n",
    "print(\"- p-values:\\n\", p_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-TkL1ygynOw"
   },
   "source": [
    "### Comments:\n",
    "\n",
    "The previous results show a p-value higher than the recommended threshold (5%) for some features. This implies that they have a low statistical relationship with the likelihood of heart disease.\n",
    "\n",
    "Next, we will use the backward elimination technique to remove variables that provide less information. In [this link](https://medium.com/@abhinav.mahapatra10/ml-basics-feature-selection-part-2-3b9b3e71c14a), you will find more information about this feature selection technique.\n",
    "\n",
    "The backward elimination technique consists of removing the least significant variables one by one, followed by repeatedly running the regression until all attributes have p-values below 0.05.\n",
    "\n",
    "Additional reference:\n",
    "* [Multiple Linear Regression (Backward Elimination Technique)](https://barcelonageeks.com/ml-regresion-lineal-multiple-tecnica-de-eliminacion-hacia-atras/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFfB5rSIz0TP"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def back_feature_elem(df, dep_var, cols):\n",
    "    \"\"\"\n",
    "    Takes the DataFrame, the dependent variable, and a list of column names.\n",
    "    Repeatedly performs logistic regression by removing the feature with the highest p-value above an alpha threshold \n",
    "    in each iteration until all p-values are below alpha, returning the final regression summary.\n",
    "    \"\"\"\n",
    "    while len(cols) > 0:  # Continue the process until no columns are left to evaluate\n",
    "        model = sm.Logit(dep_var, df[cols])  # Create a logistic model with the current columns\n",
    "        result = model.fit(disp=0)  # Fit the model without displaying the process\n",
    "        largest_pvalue = round(result.pvalues, 3).nlargest(1)  # Find the highest p-value and round to three decimals\n",
    "        if largest_pvalue.iloc[0] < 0.05:  # If the highest p-value is less than 0.05, return the result\n",
    "            return result\n",
    "        else:\n",
    "            # Remove the column with the highest p-value from the list of columns\n",
    "            cols.remove(largest_pvalue.index[0])  # Remove the column by name\n",
    "\n",
    "# Assuming 'df_constant' is your DataFrame and 'df' is another DataFrame with the dependent variable\n",
    "if 'const' not in df_constant.columns:\n",
    "    df_constant['const'] = 1\n",
    "cols = df_constant.columns.tolist()  # Assuming df_constant is defined and contains the appropriate columns\n",
    "\n",
    "def calculate_vif(df, cols):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = cols\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df[cols].values, i) for i in range(len(cols))]\n",
    "    return vif_data\n",
    "\n",
    "# Calculate VIF\n",
    "vif_df = calculate_vif(df_constant, cols)\n",
    "print(vif_df)\n",
    "\n",
    "# Remove variables with very high VIF (e.g., VIF > 10)\n",
    "cols = [col for col in cols if vif_df[vif_df['feature'] == col]['VIF'].values[0] < 10]\n",
    "\n",
    "result = back_feature_elem(df_constant, df['CLASS'], cols)\n",
    "print(result.summary())  # Display the summary of the final model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcSz1s9yFytg"
   },
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvTJlbYW-iqS"
   },
   "outputs": [],
   "source": [
    "# Select features to train the model based on the previous results\n",
    "\n",
    "X = df[relevant_features]  # Feature matrix\n",
    "y = df['CLASS']  # Class vector (target variable)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=5)\n",
    "\n",
    "# Print the shapes and first few values of the training data\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"First ten values of y_train:\", y_train[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV7MxcLjAQB5"
   },
   "source": [
    "### We train and evaluate the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXCUNS3RAVPx"
   },
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(50 * \"*\")\n",
    "print(\"\\n => Model Accuracy: => {:.2f}\".format(acc))\n",
    "\n",
    "# Display a more detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m505tqatIJNm"
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvwCMb-cIwTw"
   },
   "source": [
    "## Results and Model Evaluation\n",
    "\n",
    "Based on the model's evaluation results, several conclusions can be drawn regarding its performance. Below is a breakdown of the results and their implications:\n",
    "\n",
    "### Results:\n",
    "\n",
    "1. **Model Accuracy**: 0.70\n",
    "   - **Description**: The model achieves 70% accuracy, meaning it correctly classifies 70% of the test instances.\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - **Classes**:\n",
    "     - `0`: Does not have liver disease.\n",
    "     - `1`: Has liver disease.\n",
    "\n",
    "   - **Metrics per Class**:\n",
    "     - **Precision**:\n",
    "       - Class `0`: 0.58\n",
    "       - Class `1`: 0.71\n",
    "     - **Recall**:\n",
    "       - Class `0`: 0.19\n",
    "       - Class `1`: 0.94\n",
    "     - **F1-Score**:\n",
    "       - Class `0`: 0.29\n",
    "       - Class `1`: 0.81\n",
    "     - **Support**:\n",
    "       - Class `0`: 37 instances\n",
    "       - Class `1`: 80 instances\n",
    "\n",
    "   - **Averages**:\n",
    "     - **Macro Average**:\n",
    "       - Precision: 0.65\n",
    "       - Recall: 0.56\n",
    "       - F1-Score: 0.55\n",
    "     - **Weighted Average**:\n",
    "       - Precision: 0.67\n",
    "       - Recall: 0.70\n",
    "       - F1-Score: 0.64\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Overall Performance**:\n",
    "   - The model achieves 70% accuracy, indicating that it correctly predicts 70% of the cases overall.\n",
    "\n",
    "2. **Performance by Class**:\n",
    "   - **Class `0` (Does not have liver disease)**:\n",
    "     - Precision is low (0.58), meaning that when the model predicts no liver disease, it is correct 58% of the time.\n",
    "     - Recall is very low (0.19), indicating the model identifies only 19% of patients who truly do not have liver disease.\n",
    "     - The F1-Score is poor (0.29), reflecting a lack of balance between precision and recall for this class.\n",
    "   - **Class `1` (Has liver disease)**:\n",
    "     - Precision is high (0.71), meaning the model is correct 71% of the time when predicting liver disease.\n",
    "     - Recall is very high (0.94), indicating the model identifies 94% of patients who truly have liver disease.\n",
    "     - The F1-Score is strong (0.81), reflecting a good balance between precision and recall for this class.\n",
    "\n",
    "3. **Class Imbalance**:\n",
    "   - There is a class imbalance in the dataset (37 instances for `0` and 80 for `1`), which likely affects the performance metrics.\n",
    "   - The model appears better suited to identifying patients with liver disease (`1`) than those without it (`0`).\n",
    "\n",
    "### Implications:\n",
    "\n",
    "- **Need for Adjustment**: The poor performance on Class `0` suggests that the model might benefit from adjustments such as class-balancing techniques (e.g., undersampling, oversampling) or threshold tuning.\n",
    "- **Risk Evaluation**: In medical contexts, correctly identifying patients with a disease is critical. However, the model must also improve its ability to identify patients without the disease to minimize false positives.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Model Adjustment**: Apply techniques to handle class imbalance and improve precision and recall for Class `0`.\n",
    "- **Cross-Validation**: Use cross-validation to obtain a more robust assessment of the model's overall performance.\n",
    "- **Explore Alternative Models**: Evaluate other classification algorithms that might better handle class imbalance and improve overall performance.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "While the model performs well for identifying patients with liver disease, it requires significant improvement to correctly classify those without the disease. Further adjustments and evaluations are essential to enhance its reliability in both categories.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsQZVs+uMR7G16ZlSzHYR8",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
