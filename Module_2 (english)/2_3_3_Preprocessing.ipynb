{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_3_Preprocesado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr6zuyWq9ZjS"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step in the development of machine learning models and encompasses several fundamental processes:\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - **Handling Missing Values**: Filling missing values with a specific value, mean, median, etc.\n",
    "   - **Removing Duplicates**: Deleting or handling duplicate observations.\n",
    "   - **Outlier Removal**: Identifying and handling outliers that can skew the model.\n",
    "\n",
    "2. **Data Transformation**:\n",
    "   - **Normalization and Scaling**: Rescaling data so all values fall within a similar range.\n",
    "   - **Encoding Categorical Variables**: Converting categories into a form the model can understand, such as One-Hot Encoding.\n",
    "   - **Feature Transformation**: Creating new features from existing ones, such as variable combinations, polynomial functions, etc.\n",
    "\n",
    "3. **Dimensionality Reduction**:\n",
    "   - **Feature Selection**: Selecting the most important features that contribute to model performance.\n",
    "   - **Reduction Techniques**: Using techniques like Principal Component Analysis (PCA) to reduce data complexity without losing relevant information.\n",
    "\n",
    "4. **Data Partitioning**:\n",
    "   - **Splitting into Training, Validation, and Test Sets**: Allows model performance to be evaluated at different stages and ensures the model does not overfit the data.\n",
    "\n",
    "5. **Handling Imbalanced Data**:\n",
    "   - **Undersampling and Oversampling**: Techniques to deal with datasets where one class is overrepresented compared to another.\n",
    "\n",
    "6. **Exploratory Data Analysis (EDA)**:\n",
    "   - **Visualization**: Graphs and tables to understand the nature and relationships between variables.\n",
    "   - **Descriptive Statistics**: Provides an initial understanding of the data through metrics like mean, median, standard deviation, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdpMOqAC9UvI"
   },
   "outputs": [],
   "source": [
    "# Load rich for text enrichment\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "\n",
    "# Load common working libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load relevant preprocessing methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saF1WGpG63o6"
   },
   "outputs": [],
   "source": [
    "# We will use the Iris flower dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "r52RY00zBXEs",
    "outputId": "876b1f14-ff8f-4182-8937-5adefe948897"
   },
   "outputs": [],
   "source": [
    "console.rule(\"[blue] Information About the Working Dataset [/blue]\")\n",
    "console.log(data.head())\n",
    "console.log(f\"\\n Data Description ============= \\n\\n {data.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88xz16eE8hq8"
   },
   "source": [
    "### Generating NaNs for the Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rUGjlbyF7g3M",
    "outputId": "4133d2f5-6e79-4fe1-e945-7e0a045a3aa8"
   },
   "outputs": [],
   "source": [
    "# Introduce some missing data randomly\n",
    "\n",
    "def generate_nans(data):\n",
    "    np.random.seed(42)  # Seed for reproducibility\n",
    "\n",
    "    # Fraction of values to convert to NaN (e.g., 5%)\n",
    "    fraction = 0.05\n",
    "\n",
    "    # Mask to select entries\n",
    "    mask = np.random.rand(*data.shape) < fraction\n",
    "\n",
    "    # Convert selected entries to NaN\n",
    "    data[mask] = np.nan\n",
    "\n",
    "# Generate missing data\n",
    "generate_nans(data)\n",
    "\n",
    "# Check if there are missing values\n",
    "missing_values = data.isna()\n",
    "console.log(f\"\\n - Missing data as NaNs:\\n\", missing_values)\n",
    "\n",
    "# You can also use isnull() (Equivalent to the above, but isna() is recommended)\n",
    "missing_values = data.isnull()\n",
    "console.log(f\"\\n - Missing data as nulls:\\n\", missing_values)\n",
    "\n",
    "# Count how many missing values or NaNs are in the dataset:\n",
    "console.rule(\"Missing Data\")\n",
    "console.log(\"Number of missing values:\\n\", data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UP0Iye828qv"
   },
   "source": [
    "### Removing Rows or Columns with Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "WkLGqjV--VFu",
    "outputId": "e28b8e70-777f-44b9-bde3-e50177e2d073"
   },
   "outputs": [],
   "source": [
    "# Removing rows with at least one missing value\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Removing columns with at least one missing value\n",
    "data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Verify that there are no more missing values:\n",
    "console.rule(\"Missing Data\")\n",
    "console.log(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaIxAOXr3DIX"
   },
   "source": [
    "### Filling Missing Values with Mean, Median, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "1kzUpyF16ZNi",
    "outputId": "75f991d0-7a99-4aae-9347-3f68ab4aa895"
   },
   "outputs": [],
   "source": [
    "# Generate NaNs (only for this demonstration)\n",
    "generate_nans(data)\n",
    "console.rule(\"Initial Data\")\n",
    "console.log(data.isna().sum())\n",
    "\n",
    "# Filling with mean\n",
    "data['sepal length (cm)'] = data['sepal length (cm)'].fillna(data['sepal length (cm)'].mean())\n",
    "data['sepal width (cm)'] = data['sepal width (cm)'].fillna(data['sepal width (cm)'].mean())\n",
    "\n",
    "# Filling with median\n",
    "data['petal length (cm)'] = data['petal length (cm)'].fillna(data['petal length (cm)'].median())\n",
    "\n",
    "# Verify result\n",
    "console.rule(\"Final Data\")\n",
    "console.log(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtEMGt_v_kDn",
    "outputId": "2619b895-709b-4818-b53b-335b7615020e"
   },
   "outputs": [],
   "source": [
    "### 2. Removing Duplicates\n",
    "number_of_duplicates = data.duplicated().sum()\n",
    "print(f\"Number of duplicate entries: {number_of_duplicates}\")\n",
    "data.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UecitqCs3dd3"
   },
   "source": [
    "### Removing Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "igzGFGurAFab",
    "outputId": "dd9a1e41-1285-4d25-f61e-c6f3595eabbf"
   },
   "outputs": [],
   "source": [
    "# Reload the working data\n",
    "iris = load_iris()\n",
    "\n",
    "# Generate a reference dataset (ini) for demonstration purposes\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "data_ini = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "\n",
    "def generate_outliers(data: pd.DataFrame):\n",
    "    \"\"\" Outlier Generator \"\"\"\n",
    "    # Introduce outliers randomly (e.g., in 5% of the data)\n",
    "    fraction_outliers = 0.05\n",
    "    data_outliers = np.copy(data)\n",
    "    mask_outliers = np.random.rand(*data_outliers.shape) < fraction_outliers\n",
    "    data_outliers[mask_outliers] += 5 * data_outliers.std()\n",
    "    return data_outliers\n",
    "\n",
    "data_outliers = generate_outliers(data)\n",
    "\n",
    "# Visualize the outliers\n",
    "fig = plt.figure(1, figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.boxplot(data=data_ini, ax=ax1)\n",
    "ax1.set_title(\"Original Data\")\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.boxplot(data=data_outliers, ax=ax2)\n",
    "ax2.set_title(\"Data with Outliers\")\n",
    "plt.show()\n",
    "\n",
    "# Restructure the data\n",
    "data_outliers = pd.DataFrame(data_outliers, columns=data.columns)\n",
    "\n",
    "# Define validity range using the Interquartile Range (IQR)\n",
    "Q1 = data_outliers['sepal length (cm)'].quantile(0.25)\n",
    "Q3 = data_outliers['sepal length (cm)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the boundaries for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove the outliers\n",
    "data_outliers = data_outliers[\n",
    "    (data_outliers['sepal length (cm)'] >= lower_bound) &\n",
    "    (data_outliers['sepal length (cm)'] <= upper_bound)\n",
    "]\n",
    "\n",
    "# Visualize the result after removing outliers\n",
    "fig = plt.figure(2, figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.boxplot(data=data_outliers, ax=ax)\n",
    "ax.set_title(\"Data after Removing Outliers\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv6fN29xy4Xl"
   },
   "source": [
    "## Normalization\n",
    "\n",
    "### StandardScaler\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "* [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "VeATPZROCRLl",
    "outputId": "3ebb2867-b030-4e17-b78e-e7cf3044d44f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will use scikit-learn to apply the StandardScaler preprocessing technique.\n",
    "The goal is to transform the data so that it has a mean of zero and a unit standard deviation.\n",
    "\"\"\"\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "\n",
    "# Use the standard scaler from Scikit-learn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the data considering the mean and standard deviation of each variable\n",
    "# The \"fit\" method adjusts the model to the original data.\n",
    "scaler.fit(data.values)\n",
    "\n",
    "# Use the \"transform\" function from the StandardScaler class to apply\n",
    "# the transformation to the original data. The result of this transformation\n",
    "# is stored in the \"data_norm\" variable.\n",
    "data_norm = scaler.transform(data.values)\n",
    "\n",
    "# Verify the result\n",
    "console.log(\"Initial Data:\\n\", data[:10])\n",
    "console.log(\"Scaled Data:\\n\", data_norm[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vORh1Zufy70I",
    "outputId": "cc0784d6-237f-4b3f-899f-6d742f79c92e"
   },
   "outputs": [],
   "source": [
    "# Let's see an example with more interpretable data\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'Feature2': [10.0, 20.0, 30.0, 40.0, 50.0],\n",
    "    'Feature3': [100.0, 200.0, 300.0, 400.0, 500.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the data considering the mean and standard deviation of each variable\n",
    "# The \"fit\" method adjusts the model to the original data.\n",
    "scaler.fit(df.values)\n",
    "\n",
    "# Use the \"transform\" function from the StandardScaler class to apply\n",
    "# the transformation to the original data. The result of this transformation\n",
    "# is stored in the variable \"X_scaled\".\n",
    "X_scaled = scaler.transform(df.values)\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=df.columns)\n",
    "print(\"\\nScaled DataFrame:\\n\", df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZR38vODJSiU"
   },
   "source": [
    "### MinMaxScaler\n",
    "Transforms features by scaling them to a specified range.  \n",
    "This estimator scales and translates each feature individually so that it is within the given range in the training set, for example, between zero and one.\n",
    "\n",
    "* [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NF-H33GuJU6Y",
    "outputId": "370bfeda-d19d-4b31-b395-0ba7b762f242"
   },
   "outputs": [],
   "source": [
    "# Reload the original data to avoid previous transformations (for demonstration purposes only)\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "\n",
    "# Create the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Select columns with numerical values of interest\n",
    "columns_to_scale = ['sepal length (cm)', 'sepal width (cm)']\n",
    "data_s = data[columns_to_scale]\n",
    "\n",
    "# Print the original data\n",
    "print(\" => Original Data: \\n\", data_s.head())\n",
    "\n",
    "# Apply the scaling function\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data_s), columns=columns_to_scale)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\" => Scaled Data: \\n\", data_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "tg3ZOFrnLIaP",
    "outputId": "2093416a-128e-4540-812f-019e6e0b5ec4"
   },
   "outputs": [],
   "source": [
    "# Standard normalization of the data, also known as Z-scale or Z-standardization\n",
    "data_norm = (data_scaled - data_scaled.mean()) / data_scaled.std()\n",
    "\n",
    "### Data Visualization\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 5))\n",
    "\n",
    "# Original data histogram\n",
    "plt.subplot(131)\n",
    "sns.histplot(data['sepal length (cm)'].values, bins=20, color='blue', alpha=0.7)\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Scaled data histogram\n",
    "plt.subplot(132)\n",
    "sns.histplot(data_scaled['sepal length (cm)'].values, bins=20, color='green', alpha=0.7)\n",
    "plt.title(\"Scaled\")\n",
    "\n",
    "# Normalized data histogram\n",
    "plt.subplot(133)\n",
    "sns.histplot(data_norm['sepal length (cm)'].values, bins=20, color='orange', alpha=0.7)\n",
    "plt.title(\"Normalized\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLLKab-Q6SbC"
   },
   "source": [
    "# NOTE\n",
    "\n",
    "The following examples complement the exercises covered in the previous week:\n",
    "* Links to the respective notebooks:\n",
    "\n",
    "  * Notebook 1: 2_3_3_Preprocessing_and_Structuring_Data\n",
    "  * Notebook 2: 2_3_3_Feature_Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9DB-fna0cRt"
   },
   "source": [
    "## Encoding Categorical Variables\n",
    "Most Machine Learning algorithms require all variables to be numeric. Therefore, categorical variables must be encoded into a numerical format before being input into the model during the training (and/or inference) process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmKCq8KV0fZc",
    "outputId": "53ea7d18-360a-4f9b-8abb-161298f7880c"
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Type': ['Apple', 'Orange', 'Banana', 'Orange', 'Apple'],\n",
    "    'Color': ['Red', 'Orange', 'Yellow', 'Orange', 'Green']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# One-Hot Encoding using pandas' get_dummies\n",
    "# Create dummy variables for the categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['Type', 'Color'], prefix=['Type', 'Color'])\n",
    "\n",
    "# Display the DataFrame with encoded variables\n",
    "print(\"\\nDataFrame with Encoded Variables:\\n\", df_encoded)\n",
    "\n",
    "# Numerical encoding using pandas' factorize\n",
    "# Encode the categorical variable 'Type' numerically\n",
    "df['Type_encoded'] = pd.factorize(df['Type'])[0]\n",
    "\n",
    "# Display the DataFrame with the 'Type' variable numerically encoded\n",
    "print(\"\\nDataFrame with 'Type' Variable Numerically Encoded:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KleiyiyQMqkB"
   },
   "source": [
    "# Dimensionality Reduction (Feature Extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ8IllEz9fcg"
   },
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "Principal Component Analysis, or PCA, is a statistical technique used to reduce the dimensionality of a dataset. PCA allows us to simplify the information present in a dataset with multiple variables and transform it into a reduced dataset that still retains much of the original information.\n",
    "\n",
    "The goal of PCA is to find a representation of the data that is easier to understand while preserving as much variance in the data as possible.\n",
    "\n",
    "To perform PCA, the covariance matrix of the original data is first calculated. Then, the eigenvectors of this matrix are computed, which indicate the directions in which the data has the greatest variance. The original data is then projected onto these directions, resulting in a new dataset with fewer variables that still captures a significant portion of the original information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "685FWKeHDF6p",
    "outputId": "543328b0-5dd4-44b8-cc3b-daf9590e8cf0"
   },
   "outputs": [],
   "source": [
    "console.rule(\"[blue] Principal Component Analysis [/blue]\")\n",
    "\n",
    "# Use Sci-kit learn functions for PCA analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the following data for this example\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "\n",
    "# Display a summary of the dataset\n",
    "console.log(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "5vBzhnkiNTm-",
    "outputId": "f92e22be-cd0d-419c-9372-3dbbd74dc067"
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.values)\n",
    "data_scaled = scaler.transform(df.values)\n",
    "\n",
    "console.log(\"Number of features:\", data_scaled.shape[1])\n",
    "\n",
    "# To evaluate the results, use the full set of variables\n",
    "# \"n_components\" = 30 specifies that PCA should fit the data to find\n",
    "# the 30 principal components.\n",
    "pca = PCA(n_components=30, random_state=2020)\n",
    "pca.fit(data_scaled)\n",
    "\n",
    "# Store the values of the (30) principal components in the variable X_pca\n",
    "X_pca = pca.transform(data_scaled)\n",
    "print(\"X_pca:\\n\", X_pca)\n",
    "\n",
    "# Since the full set of variables was selected, the chosen components\n",
    "# should account for 100% of the variance in the data\n",
    "print(\"\\n => Variance explained by the components:\", sum(pca.explained_variance_ratio_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "0C-Vj6zTFOyn",
    "outputId": "7ddd7b36-c91f-4182-9644-a36bdf547927"
   },
   "outputs": [],
   "source": [
    "# By plotting the variance as a function of the number of components, we can observe\n",
    "# the minimum number of components needed to explain a certain percentage of the variance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained by PCA Components\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjfIN5w5GPf3",
    "outputId": "b65a4ce8-1ba9-43ff-994f-3da506fbfee6"
   },
   "outputs": [],
   "source": [
    "# We see that with just a third of the variables, we can explain 95% of the variance\n",
    "n_var = np.cumsum(pca.explained_variance_ratio_ * 100)[9]\n",
    "print(\"Variance explained by the first 10 components:\", n_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "yhsrw_9xOCpk",
    "outputId": "333217fc-6543-44b3-9cd6-e14d5db84cea"
   },
   "outputs": [],
   "source": [
    "# Alternatively, we can construct the set that accommodates 95% of the variance\n",
    "# as follows\n",
    "\n",
    "# Fit PCA to retain 95% of the variance\n",
    "pca_95 = PCA(n_components=0.95, random_state=2020)\n",
    "pca_95.fit(data_scaled)\n",
    "X_pca_95 = pca_95.transform(data_scaled)\n",
    "\n",
    "# Convert the first two principal components into a DataFrame for better visualization\n",
    "pca_df = pd.DataFrame(X_pca_95[:, :2], columns=['PC1', 'PC2'])\n",
    "\n",
    "# Assuming 'cancer_data.target' is a Series or list with target labels, add it to the DataFrame\n",
    "pca_df['Target'] = cancer_data.target\n",
    "\n",
    "# Visualize the relationship of the first two principal components with a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target', palette='viridis')\n",
    "plt.title('Visualization of the First Two Principal Components')\n",
    "plt.xlabel('First Principal Component (PC1)')\n",
    "plt.ylabel('Second Principal Component (PC2)')\n",
    "plt.legend(title='Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx7Yp5VjOhre"
   },
   "source": [
    "### Notes\n",
    "The scatter plot generated from the first two principal components (PC1 and PC2) of the PCA aims to represent the underlying structure of the data in a lower-dimensional space. This can reveal information about relationships among observations and differences between classes (in this case, represented by `cancer_data.target`).\n",
    "\n",
    "* **Distinct Clusters**: If observations from different classes form distinct and clearly separated clusters in the plot, this indicates significant differences between the classes. This suggests that the original features contain valuable information for class differentiation.\n",
    "\n",
    "* **Overlapping Clusters**: If the clusters overlap but there is still some separation, this suggests that there is some information in the data to differentiate the classes, but it is not very clear. There may be features contributing noise, or the separation might require more than two dimensions to become fully visible.\n",
    "\n",
    "* **Absence of Clusters**: If there is no clear separation between classes and the observations are mixed, this might indicate that the original features are not very useful for class differentiation, at least in the first two principal components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pe86_D2lWqb"
   },
   "source": [
    "## Independent Component Analysis (ICA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qP5KjpDTlSHz",
    "outputId": "d1c55c55-2fef-486b-a1ac-e27db9c761a4"
   },
   "outputs": [],
   "source": [
    "# We will use fMRI data for our example with ICA\n",
    "# To do this, we start by installing the nilearn library\n",
    "!python -m pip install nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD8hbq5mmD2s",
    "outputId": "f879bae7-d65d-4833-ce18-109154962645"
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# Download a subject from the functional MRI study\n",
    "dataset = datasets.fetch_development_fmri(n_subjects=1)\n",
    "file_name = dataset.func[0]\n",
    "\n",
    "# Image preprocessing\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# Apply a mask to extract the background from the image (non-brain voxels)\n",
    "masker = NiftiMasker(smoothing_fwhm=8, memory='nilearn_cache', memory_level=1,\n",
    "                     mask_strategy='epi', standardize=True)\n",
    "data_masked = masker.fit_transform(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs474WXDmZQZ",
    "outputId": "0d7e9a4a-1834-44c0-a16b-120d473ad581"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "import numpy as np\n",
    "\n",
    "# Select 10 components\n",
    "ica = FastICA(n_components=10, random_state=42)\n",
    "components_masked = ica.fit_transform(data_masked.T).T\n",
    "\n",
    "# Apply a threshold (80% signal) to the data after normalization\n",
    "# based on mean and standard deviation\n",
    "components_masked -= components_masked.mean(axis=0)\n",
    "components_masked /= components_masked.std(axis=0)\n",
    "components_masked[np.abs(components_masked) < .8] = 0\n",
    "\n",
    "# Invert the transformation to recover the 3D structure\n",
    "component_img = masker.inverse_transform(components_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "id": "y8u270f2nEm-",
    "outputId": "aea58c2d-33cb-482a-ced6-5e45877bbb8e"
   },
   "outputs": [],
   "source": [
    "# Finally, visualize the results of the dimensionality reduction operations\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "mean_img = image.mean_img(file_name)\n",
    "plot_stat_map(image.index_img(component_img, 0), mean_img)\n",
    "plot_stat_map(image.index_img(component_img, 1), mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4CwMGhaPZhU"
   },
   "outputs": [],
   "source": [
    "# Export the data for analysis with another tool...\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "df.to_csv(\"sample_data/cancer_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUUQgkNsAgSqik8q1nkm5Z",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
