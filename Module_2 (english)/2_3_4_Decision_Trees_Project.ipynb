{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_4_Proyecto_Arboles_de_decision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIVHTg9n5abF"
   },
   "source": [
    "# Decision Trees\n",
    "Application in a classification problem using data related to morphologies and other clinically relevant features in breast cancer diagnosis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqMpqH975doP"
   },
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DTfU444S3uZR",
    "outputId": "ff05fdcb-a20c-4143-d441-1ee220355e5e"
   },
   "outputs": [],
   "source": [
    "# Load Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "\n",
    "sns.set(style=\"whitegrid\") # Configure Seaborn figure style\n",
    "\n",
    "# This dataset is part of Scikit-learn's example datasets\n",
    "# Reference: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset\n",
    "data = load_breast_cancer()\n",
    "# The DESCR method provides a docstring with information about the dataset\n",
    "print(data.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM78_K4k7W80"
   },
   "source": [
    "## Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_BylzQiI7bCw",
    "outputId": "9ac157a6-4b66-4fb1-a48e-997ab3f96ce7"
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from the dataset dictionary\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Check for missing values per variable\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Number of missing values per variable:\\n\", missing_values)\n",
    "\n",
    "# Visualize the distribution of missing values\n",
    "plt.figure(figsize=(12, 8))  # Adjust figure size for better readability\n",
    "sns.heatmap(df.isnull(), cbar=True, cmap='coolwarm', yticklabels=False)\n",
    "plt.title(\"Distribution of Missing Values in the Dataset\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Entries\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels to prevent overlap\n",
    "plt.grid(False)  # Optional: set to True if grid lines are preferred\n",
    "plt.tight_layout()  # Automatically adjust subplot parameters\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "I_5KtBH_WLZk",
    "outputId": "d8ba0150-db81-4a78-d975-3aebef484fc9"
   },
   "outputs": [],
   "source": [
    "# Function to introduce missing values in a DataFrame\n",
    "def add_missing_values(df, missing_percentage=0.05):\n",
    "    # Calculate the total number of cells in the DataFrame\n",
    "    total_cells = np.product(df.shape)\n",
    "    # Calculate the total number of cells that need to be NaN\n",
    "    total_missing = int(total_cells * missing_percentage)\n",
    "\n",
    "    # Get the indices of rows and columns for the cells to be turned into NaN\n",
    "    row_indices = np.random.choice(df.shape[0], total_missing)\n",
    "    col_indices = np.random.choice(df.shape[1], total_missing)\n",
    "\n",
    "    # Assign NaN to the selected cells\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        df.iat[row, col] = np.nan\n",
    "\n",
    "# Create a copy of the DataFrame and add missing values\n",
    "df_missing = df.copy()\n",
    "add_missing_values(df_missing, missing_percentage=0.01)  # Add 1% of missing values\n",
    "\n",
    "# Visualize the missing values introduced\n",
    "plt.figure(figsize=(12, 8))  # Adjust figure size for better readability\n",
    "sns.heatmap(df_missing.isnull(), cbar=True, cmap='coolwarm', yticklabels=False)\n",
    "plt.title(\"Distribution of Missing Values in the Dataset\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Entries\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels to prevent overlap\n",
    "plt.grid(False)  # Optional: set to True if grid lines are preferred\n",
    "plt.tight_layout()  # Automatically adjust subplot parameters\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 39
    },
    "id": "UoS9KJjb86lj",
    "outputId": "dc5dd651-253b-42b4-a688-41619c76d980"
   },
   "outputs": [],
   "source": [
    "# The goal of the project is to achieve high accuracy in classifying\n",
    "# examples in the test set into the two categories indicating whether\n",
    "# the tumor is malignant or benign.\n",
    "print(\"Target Classes:\", data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "FyYyHep_zG5v",
    "outputId": "57cf97a8-bcc4-4679-e592-763f15e9045e"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Visualize the class distribution in the target variable\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.countplot(x=y, ax=ax, palette=\"viridis\")  # Using the 'viridis' color palette\n",
    "ax.set_title(\"Class Distribution in Target Variable\")\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xticklabels(data.target_names)\n",
    "\n",
    "# Add data labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='bottom', color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5qf-BoIKzJn"
   },
   "source": [
    "## Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "Cw9cfVpOK1aI",
    "outputId": "97df0341-6090-4ac5-911f-d166bde2b598"
   },
   "outputs": [],
   "source": [
    "labels = data.target\n",
    "\n",
    "# Count the number of instances per class\n",
    "class_counts = np.bincount(labels)\n",
    "class_counts, class_counts / len(labels) * 100\n",
    "class_labels = data.target_names\n",
    "class_percentages = class_counts / len(labels) * 100\n",
    "\n",
    "print(\"Class Counts:\", class_counts)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_labels, class_percentages, color=['red', 'green'])\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Class Balance in Breast Cancer Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.ylim(0, 100)  # Set limits for y-axis for better clarity\n",
    "\n",
    "# Display percentages on the bars\n",
    "for i, percentage in enumerate(class_percentages):\n",
    "    plt.text(i, percentage + 1, f'{percentage:.2f}%', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPPyIQFyLz_4"
   },
   "source": [
    "**Conclusions**: The dataset exhibits a relative imbalance, with a higher proportion of samples classified as cancer absence (class 1) compared to cancer presence (class 0). However, the difference is not extreme. With approximately 37% positive cases and 63% negative cases, standard machine learning techniques might handle this imbalance without specific adjustments.\n",
    "\n",
    "For algorithms sensitive to class imbalance or in scenarios where misclassifying one class has significantly more severe consequences, strategies to balance the distribution could be considered. These strategies include oversampling the minority class, undersampling the majority class, or applying synthetic data generation methods such as SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uhffthqC_c_R",
    "outputId": "24ab9ea4-b052-4d3f-bb77-8c3925f3e096"
   },
   "outputs": [],
   "source": [
    "# Increase the font size\n",
    "sns.set_context('talk', font_scale=0.7)\n",
    "\n",
    "# Create a figure to visualize the correlation matrix with an adjusted size\n",
    "plt.figure(figsize=(20, 20))  # Adjust the figure size\n",
    "\n",
    "# Generate a heatmap of the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Set titles and axis labels\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Variables')\n",
    "\n",
    "# Rotate x-axis labels and improve subplot layout\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Automatically adjust subplot parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "me0gYI4bBlUv",
    "outputId": "73eefcad-c8e7-43ce-9b3d-9ba42da32457"
   },
   "outputs": [],
   "source": [
    "# In the previous graph, we observed that several variables exhibit high correlation. Let's explore how we can remove them.\n",
    "\n",
    "# Define a function to identify columns with correlation below a certain threshold (thres)\n",
    "def find_lc_cols(df, thres):\n",
    "    \"\"\"\n",
    "    Finds variables with correlation greater than the threshold (thres).\n",
    "    \"\"\"\n",
    "    corr = df.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if abs(corr.iloc[i, j]) >= thres:\n",
    "                if columns[j]: \n",
    "                    columns[j] = False\n",
    "\n",
    "    return columns\n",
    "\n",
    "# Run the function and retrieve columns/variables with low correlation\n",
    "lc_cols = find_lc_cols(df, thres=0.90)\n",
    "print(\"Variables with low (<90%) correlation:\", df.columns[lc_cols].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aUblKQpF0DqP",
    "outputId": "46c2fc5d-948d-46e0-8825-6d593e771176"
   },
   "outputs": [],
   "source": [
    "# Build a DataFrame that includes only the selected columns\n",
    "s_cols = df.columns[lc_cols]\n",
    "df_s = df[s_cols]\n",
    "\n",
    "# Log the results\n",
    "console.log(f\"Selected variables: {len(df_s.columns)}\", style=\"bold blue\")\n",
    "console.log(f\"From a total of: {len(df.columns)}\", style=\"bold blue\")\n",
    "console.log(f\"\\nFinal dataset: \\n{df_s}\", style=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "oQCtJZyTLDTt",
    "outputId": "6fa98f4e-6b27-49fc-c9e3-77a21d6f8b66"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets using Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_s[df_s.columns]  # Features\n",
    "y = data.target  # Target labels\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(\" - X_train:\", X_train.shape)\n",
    "print(\" - X_test:\", X_test.shape)\n",
    "print(\" - y_train:\", y_train.shape)\n",
    "print(\" - y_test:\", y_test.shape)\n",
    "\n",
    "# Visualize the distribution of the training and testing sets\n",
    "shapes = {\n",
    "    'X_train': X_train.shape[0],\n",
    "    'y_train': y_train.shape[0],\n",
    "    'X_test': X_test.shape[0],\n",
    "    'y_test': y_test.shape[0]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(shapes.keys(), shapes.values(), color=['blue', 'orange', 'green', 'red'])\n",
    "plt.xlabel('Data Sets')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Distribution of Training and Testing Sets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dd_w07OL9W6"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "xM_BZb5QL84-",
    "outputId": "78a1486e-616a-4516-b7ca-fd70b3be4f06"
   },
   "outputs": [],
   "source": [
    "# It's time to configure our classifier and fit the model to the data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(criterion='gini', max_depth=2)\n",
    "clf_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 39
    },
    "id": "fD51QHyTMOFi",
    "outputId": "ca0a89c3-231c-44f2-a8c6-fdf9fcd05d6b"
   },
   "outputs": [],
   "source": [
    "# Fitting the model, let's evaluate its performance\n",
    "# Calculate the predictions of the trained model on the test set\n",
    "y_test_pred = clf_dt.predict(X_test)\n",
    "\n",
    "# Using the true labels in the test set, calculate the model's accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(\"The model's accuracy is: {:0.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "r0DlPtn_2dz3",
    "outputId": "d9f16be9-104e-4903-ce0d-6de3f4584fb2"
   },
   "outputs": [],
   "source": [
    "# Let's now look at the confusion matrix for a better assessment of performance.\n",
    "# Remember that the test set includes 114 subjects.\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Create an instance of ConfusionMatrixDisplay to visualize the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Set the title and display the confusion matrix\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "HAhA4inxZlXP",
    "outputId": "1e7d93f3-83ad-4b39-b738-8733017c0468"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming `y_test` is your set of true labels and `y_test_pred` are the model predictions\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Create an instance of ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=data.target_names)\n",
    "\n",
    "# Configure and display the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "disp.plot(cmap='Blues', values_format='.2f')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "\n",
    "# Annotate additional metrics\n",
    "plt.figtext(0.5, -0.1, f'Precision: {precision:.2f}', ha='center', va='center')\n",
    "plt.figtext(0.5, -0.15, f'Recall/Sensitivity: {recall:.2f}', ha='center', va='center')\n",
    "plt.figtext(0.5, -0.2, f'F1-Score: {f1:.2f}', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust subplot parameters\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sEVeMT5Mjki"
   },
   "source": [
    "## Results\n",
    "The results obtained highlight the robustness of our model, demonstrating a classification accuracy exceeding 91%. Furthermore, it successfully identifies 67 malignant cases and 37 benign cases accurately. Although it stands out for its performance, 6 false positives and 4 false negatives have been recorded, providing valuable insights into areas that could be refined to further enhance its effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifaZCVFeAz5n"
   },
   "source": [
    "## Apply Logistic Regression Model\n",
    "Let's see how well a logistic regression model performs.\n",
    "\n",
    "https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 39
    },
    "id": "ci6htfX9AGj1",
    "outputId": "0468629b-7608-4059-98fb-d0af88aa52f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and fit the Logistic Regression model\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = clf_lr.predict(X_test)\n",
    "\n",
    "# Calculate and display the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(\"The model accuracy (Logistic Regression) is: {:.2f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73V-d-P6Bh-X"
   },
   "source": [
    "## ROC Curve\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical tool used to evaluate classification models. It provides a visual representation of how well a model can distinguish between two distinct classes by varying the decision threshold.\n",
    "\n",
    "The curve is created by plotting the True Positive Rate (Sensitivity) on the y-axis and the False Positive Rate (1 - Specificity) on the x-axis. Here's an explanation of the key terms:\n",
    "\n",
    "- **Sensitivity (True Positive Rate - TPR):** Measures the proportion of positive instances correctly classified as positive by the model. It is calculated as TP / (TP + FN), where TP is the number of true positives (correctly classified positive instances), and FN is the number of false negatives (positive instances incorrectly classified as negative).\n",
    "\n",
    "- **Specificity (True Negative Rate - TNR):** Measures the proportion of negative instances correctly classified as negative by the model. It is calculated as TN / (TN + FP), where TN is the number of true negatives (correctly classified negative instances), and FP is the number of false positives (negative instances incorrectly classified as positive).\n",
    "\n",
    "On the ROC curve, each point represents a different decision threshold, which affects the true positive rate and the false positive rate. An ideal model would have a ROC curve that reaches the upper-left corner (sensitivity of 1, specificity of 1), indicating no false positives and all true positives correctly classified.\n",
    "\n",
    "The Area Under the ROC Curve (AUC) is another important metric. It measures the overall discriminative ability of the model across all possible decision thresholds. An AUC close to 1 indicates a model with good discriminative power, while an AUC near 0.5 suggests performance similar to random guessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "F4gRxGV732sT",
    "outputId": "2e3429d3-8251-47b1-f31a-246a8c8febe9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the probability scores assigned by the model on the test set\n",
    "y_score = clf_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute the False Positive Rate (FPR) and True Positive Rate (TPR)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "# Calculate the Area Under the ROC Curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create a figure to visualize the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "\n",
    "# Configure axis limits, labels, and add a legend\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title('ROC Curve (Logistic Regression)')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBkqq9aoaHqO"
   },
   "source": [
    "## Other Useful Models for Classification Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oyE1nqlBaHQC",
    "outputId": "f4bc977e-99b2-4c86-92d1-b82eba125eff"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and Evaluate Each Model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Predict on the test set\n",
    "    console.rule(f\"[bold]Model {name}[/bold]\")\n",
    "    print(classification_report(y_test, y_pred))  # Display performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hdaCyxClfdu"
   },
   "source": [
    "## Comments on the Results\n",
    "\n",
    "Analyzing the performance metrics of the Random Forest-based model:\n",
    "* Class 0: Malignant tumor\n",
    "* Class 1: Benign tumor\n",
    "\n",
    "### Precision\n",
    "- **Class 0**: 0.95. This means that 95% of the time the model predicts class 0, it is correct.\n",
    "- **Class 1**: 0.96. 96% of the time the model predicts class 1, it is correct.\n",
    "- High precision in both classes suggests the model is effective at minimizing false positives.\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "- **Class 0**: 0.93. Of all actual instances of class 0, the model correctly identifies them 93% of the time.\n",
    "- **Class 1**: 0.97. The model correctly identifies 97% of all actual instances of class 1.\n",
    "- High recall indicates the model is good at identifying positive classes.\n",
    "\n",
    "### F1-Score\n",
    "- **Class 0**: 0.94. It is a balance between precision and recall for class 0.\n",
    "- **Class 1**: 0.97. It is a balance between precision and recall for class 1.\n",
    "- F1-Score is especially useful in situations where a balance between precision and recall is important. A high value in both classes indicates a good balance.\n",
    "\n",
    "### Support\n",
    "- **Class 0**: 43. There are 43 instances of class 0 in the dataset.\n",
    "- **Class 1**: 71. There are 71 instances of class 1.\n",
    "- Support indicates the number of actual occurrences of each class in the test dataset.\n",
    "\n",
    "### Overall Accuracy\n",
    "- 0.96. The model correctly classified 96% of all instances.\n",
    "\n",
    "### Macro Avg and Weighted Avg\n",
    "- **Macro Avg**: Computes the arithmetic mean of the metrics for each class, treating all classes equally. For this model, the macro averages for precision, recall, and f1-score are 0.96, 0.95, and 0.95 respectively.\n",
    "- **Weighted Avg**: Computes the weighted average of the metrics, considering the support (number of instances) for each class. The weighted averages for precision, recall, and f1-score are 0.96.\n",
    "\n",
    "### General Interpretation\n",
    "The model demonstrates high performance across all metrics for both classes, indicating effectiveness in correctly identifying classes (high recall) and minimizing false positives (high precision). The high f1-score in both classes suggests a good balance between precision and recall. The overall accuracy is also high, meaning the model is very efficient in correctly classifying instances overall. However, it is always essential to consider the application context of the model, as in some cases, the cost of false positives or false negatives could be critical and require a more nuanced approach than suggested by a high overall accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBsYDPr3rUaarEANsZ9AxN",
   "include_colab_link": true,
   "name": "2_3_4_Proyecto_Arboles_de_decision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
