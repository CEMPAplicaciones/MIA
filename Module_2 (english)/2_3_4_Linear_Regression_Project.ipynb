{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_4_Proyecto_Regresion_Lineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4ezADhid1nt"
   },
   "source": [
    "In this project, our goal is to build a simple linear regression model using sci-kit learn to predict medical costs based on the dataset available [at this link](https://www.kaggle.com/mirichoi0218/insurance).\n",
    "\n",
    "The first step will be to upload the data file to the Google virtual machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2TcO1IPUz4E"
   },
   "source": [
    "## Load Basic Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "AJLvzS0hdxR_",
    "outputId": "6557cda9-886e-45f3-da3d-1d7fb0dc6ccc"
   },
   "outputs": [],
   "source": [
    "# First, we import the basic working libraries for any machine learning project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "\n",
    "# And we load the data contained in our dataset (downloaded and uploaded)\n",
    "df = pd.read_csv(\"insurance.csv\")\n",
    "console.log(f\"DataFrame Header: {df.head()}\", style=\"bold yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiUH9FNBgeUJ"
   },
   "source": [
    "## Data Exploration\n",
    "As we have seen in Topic 2.3.3, one of the first steps we must take in any Machine Learning project is the descriptive and visual exploration of the working data. This process is essential to thoroughly understand the nature of the data we are working with and to lay the foundation upon which subsequent models will be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "IDHwSelNR3zk",
    "outputId": "94a7ff30-872e-451f-ef5e-097b222b4310"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# We observe how the target variable \"charges\" is distributed\n",
    "# which represents the insurance charges\n",
    "sns.histplot(df['charges'], stat=\"density\")\n",
    "plt.title('Distribution of Insurance Charges')\n",
    "plt.xlabel('Charges')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Fit the data to a normal distribution\n",
    "mu, std = norm.fit(df['charges'])\n",
    "\n",
    "# Plot the PDF (Probability Density Function)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2, color='red')\n",
    "plt.legend(['Normal Distribution', 'Observed Data'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print relevant statistics using rich.Console\n",
    "console.rule(\"Relevant Statistics\")\n",
    "console.print(f\"Mean of Charges: [bold]{mu:.2f}[/bold]\")\n",
    "console.print(f\"Standard Deviation of Charges: [bold]{std:.2f}[/bold]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzSNQDE5R975"
   },
   "source": [
    "The Shapiro-Wilk test is a statistical test used to assess whether a dataset follows a normal distribution. The normal distribution, also known as the Gaussian distribution, is a continuous distribution commonly observed in many natural phenomena and serves as a fundamental basis for many statistical methods and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "7OdAy-meRewN",
    "outputId": "0d919b81-63d7-41ed-b865-0e647d529399"
   },
   "outputs": [],
   "source": [
    "# Define a function to perform a normality test and check\n",
    "# the Gaussian nature (or not) of the distribution\n",
    "\n",
    "from scipy.stats import norm, shapiro\n",
    "\n",
    "def test_normality(data):\n",
    "    stat, p_value = shapiro(data)\n",
    "    console.rule(\"Normality Test\")\n",
    "    console.print(f\"Test Statistic: [bold]{stat:.4f}[/bold]\")\n",
    "    console.print(f\"P-value: [bold]{p_value:.4f}[/bold]\")\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        console.print(\"The data follows a normal distribution (p > 0.05).\")\n",
    "    else:\n",
    "        console.print(\"The data does not follow a normal distribution (p <= 0.05).\")\n",
    "\n",
    "# Perform the normality test\n",
    "test_normality(df['charges'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQYGrsHvhEWN"
   },
   "source": [
    "From the previous results, we can conclude that the target variable \"charges\" does not follow a normal distribution but rather a mixed distribution. This could present a challenge in achieving optimal performance for our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "Uy5ad-ycRjy9",
    "outputId": "4bd243b7-68c3-4367-86ca-6c31cfa0f964"
   },
   "outputs": [],
   "source": [
    "# Reviewing the correlation matrix to identify possible dependencies\n",
    "correlation_matrix = df.corr()  # Using the method implemented by Pandas\n",
    "\n",
    "# Create a figure and axis for the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation of features with \"charges,\" sorted by the highest correlation values\n",
    "console.rule(\"Features Most Correlated with 'charges'\")\n",
    "charges_correlation = correlation_matrix['charges'].sort_values(ascending=False)\n",
    "console.print(charges_correlation)\n",
    "\n",
    "# Another way to generate the heatmap using Seaborn:\n",
    "# sns.heatmap(df.corr(), annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nQvXyYmOgS5u",
    "outputId": "d824c720-fa99-45f1-a891-3d9ded246ace"
   },
   "outputs": [],
   "source": [
    "# Create a pair plot to visualize the morphology of correlations\n",
    "sns.set(style=\"ticks\")\n",
    "pair_plot = sns.pairplot(df)\n",
    "plt.suptitle(\"Pair Plot of Features\")\n",
    "plt.show()\n",
    "\n",
    "# Print a brief description of the pair plot\n",
    "console.rule(\"Pair Plot Description\")\n",
    "console.print(\"The pair plot provides a graphical representation of the relationships between the dataset's features.\")\n",
    "console.print(\"The diagonal contains individual distributions of the features.\")\n",
    "console.print(\"The off-diagonal cells display scatter plots between pairs of features, helping identify correlation patterns and trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf1ojbLzUhcf"
   },
   "source": [
    "## Encoding Categorical Variables\n",
    "The next step is to encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "id": "JmUWDPxsUhI-",
    "outputId": "c5a1accb-3c14-4e2f-c8fa-67aa4616675f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Categorical variables to encode\n",
    "vars_to_encode = ['sex', 'smoker', 'region']\n",
    "\n",
    "# Use the get_dummies method to encode categorical variables\n",
    "dummies = [pd.get_dummies(df[var], prefix=var) for var in vars_to_encode]\n",
    "df_dummies = pd.concat(dummies, axis=1)\n",
    "\n",
    "# Remove the original columns of the categorical variables\n",
    "df = df.drop(vars_to_encode, axis=1)\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df, df_dummies], axis=1)\n",
    "\n",
    "# Rename columns for better clarity\n",
    "df_encoded.rename(columns={'smoker_no': 'non-smoker', 'smoker_yes': 'nicotine-user'}, inplace=True)\n",
    "\n",
    "# Display the updated dataset with encoded variables\n",
    "console.log(f\"Encoded DataFrame: {df_encoded.head()}\", style=\"bold yellow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2_41g9qVZl0"
   },
   "source": [
    "## Data Normalization\n",
    "The next phase of preprocessing involves rescaling the data. In this case, we will apply methods from the StandardScaler class in Sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8R4bXdiqXgDI",
    "outputId": "2c8a7a67-f6b9-4cd4-9845-aee70aa9cc98"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a copy of the encoded DataFrame for scaling\n",
    "df_c_scaled = df_encoded.copy()\n",
    "\n",
    "# Use StandardScaler to scale the dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_c_scaled)\n",
    "df_c_scaled[df_c_scaled.columns] = scaled_features\n",
    "\n",
    "# Display the scaled dataset\n",
    "print(\"\\n Scaled Dataset: \\n\", df_c_scaled)\n",
    "\n",
    "# Visualize the distribution of the \"charges\" column in the scaled dataset\n",
    "sns.histplot(df_c_scaled[\"charges\"])\n",
    "plt.title(\"Distribution of Scaled Charges\")\n",
    "plt.xlabel(\"Scaled Charges\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63bgxpkgbjaR"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "vBy1rCtsboId",
    "outputId": "f9636ede-f670-4475-d256-6ec7ea7cd5ad"
   },
   "outputs": [],
   "source": [
    "# We will apply the PCA technique to identify the most representative variables\n",
    "from sklearn.decomposition import PCA\n",
    "df_s = df_c_scaled  # Our previously processed working DataFrame\n",
    "\n",
    "# The feature names are identified in the column headers\n",
    "features = df_s.columns\n",
    "X = df_s[features]\n",
    "\n",
    "# Analyze the complete set of variables\n",
    "pca = PCA(n_components=len(features), random_state=2020)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Plot the percentage of variance explained vs the number of components\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "Et3qTxSceCEq",
    "outputId": "32875428-e6db-4ba5-d18e-4eb1ac1a951c"
   },
   "outputs": [],
   "source": [
    "# Create an instance of PCA with 6 principal components\n",
    "pca_s = PCA(n_components=6, random_state=2020)\n",
    "\n",
    "# Fit the PCA model to the original data\n",
    "pca_s.fit(X)\n",
    "\n",
    "# Transform the original data using the principal components\n",
    "X_pca_s = pca_s.transform(X)\n",
    "\n",
    "# Create a new DataFrame with the transformed data\n",
    "cols = ['PCA' + str(i) for i in range(1, 7)]  # Changed to make components range from 1 to 6\n",
    "df_pca = pd.DataFrame(X_pca_s, columns=cols)\n",
    "\n",
    "# Finally, observe the dataset transformed with the 6 principal components\n",
    "console.log(f\"Data for the 6 principal components:{df_pca}\", style=\"bold yellow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDkOd6gXbA6N"
   },
   "source": [
    "## Linear Model Fitting\n",
    "After completing the data preparation process, we will proceed to fit the linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88765tFybAEt",
    "outputId": "e465f773-6ff1-4abf-ea08-f9f2ac4eb913"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The features we are analyzing are those selected with PCA\n",
    "pca_features = df_pca.columns\n",
    "X = df_pca[pca_features]\n",
    "y = df_s['charges']\n",
    "\n",
    "# Use the train_test_split function from sklearn to split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"Training and testing set sizes:\")\n",
    "print(\" - X_train:\", X_train.shape)\n",
    "print(\" - X_test:\", X_test.shape)\n",
    "print(\" - y_train:\", y_train.shape)\n",
    "print(\" - y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "nYka_jgcBgWu",
    "outputId": "0b4ecd36-5b2c-4235-bcab-d427a3785da7"
   },
   "outputs": [],
   "source": [
    "# Expose the training set to the linear regression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create an instance of the linear regression model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "lm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqlOVCEuB476",
    "outputId": "0f02d66e-1444-4cf8-898e-0e80425a9427"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the model's predictions on the test set\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "# Compute model fit and performance metrics\n",
    "evar = metrics.explained_variance_score(y_test, y_pred)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Finally, print the fit and performance metrics\n",
    "print(\"Fit and Performance Metrics:\")\n",
    "print('- Explained Variance: ', round(evar, 2))\n",
    "print('- R2:', round(r2, 2))\n",
    "print('- MAE: ', round(mae, 4))\n",
    "print('- MSE: ', round(mse, 4))\n",
    "print('- RMSE: ', round(rmse, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "BF3-YvBtGMVI",
    "outputId": "16c4e9e6-68da-4c23-b3b3-e1c9e1e11f43"
   },
   "outputs": [],
   "source": [
    "# To finish, we represent the model's predictions in a graph:\n",
    "\n",
    "# Scatter plot of predictions vs. actual values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(y_pred, y_test, color='blue', edgecolors=(0, 0, 1), label='Test Data')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3, label='45° Line')\n",
    "\n",
    "# Labels and title of the graph\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual Value')\n",
    "ax.set_title('Comparison of Predictions and Actual Values')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWwed9IBNQC/fqwW5gyLor",
   "include_colab_link": true,
   "name": "2_3_4_Proyecto_Regresion_Lineal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
