{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_3_Extraccion_de_caracteristicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr6zuyWq9ZjS"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ8IllEz9fcg"
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis, or PCA, is a statistical technique used to reduce the dimensionality of a dataset. PCA allows us to simplify the information present in a dataset with multiple variables and transform it into a reduced dataset that still retains much of the original information.\n",
    "\n",
    "The goal of PCA is to find a representation of the data that is easier to understand while preserving as much variance in the data as possible.\n",
    "\n",
    "To perform PCA, the covariance matrix of the original data is first calculated. Then, the eigenvectors of this matrix are computed, which indicate the directions in which the data has the greatest variance. The original data is then projected onto these directions, resulting in a new dataset with fewer variables that still captures a significant portion of the original information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdpMOqAC9UvI"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r52RY00zBXEs",
    "outputId": "96250221-2121-4dac-9711-cbfaf4ab6fb0"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "\n",
    "# Display a few variables\n",
    "print(df.head())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeATPZROCRLl",
    "outputId": "c1247269-bd5b-4431-f738-3a353c5b3051"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will use scikit-learn to apply the preprocessing technique StandardScaler. \n",
    "The goal is to transform the data so that it has a mean of zero and a unit standard deviation.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Rescale the data considering the mean and standard deviation of each variable\n",
    "# The \"fit\" method adjusts the model to the original data.\n",
    "scaler.fit(df.values)\n",
    "\n",
    "# Use the \"transform\" function from the StandardScaler class to apply \n",
    "# the transformation to the original data. The result of this transformation \n",
    "# is stored in the variable \"X_scaled\"\n",
    "X_scaled = scaler.transform(df.values)\n",
    "print(\"X_scaled:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "685FWKeHDF6p",
    "outputId": "5ca38cb5-f507-426e-bd2d-550680a3a398"
   },
   "outputs": [],
   "source": [
    "# We will use scikit-learn functions for PCA analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# To evaluate the results, we will use the full set of variables.\n",
    "# \"n_components = 30\" specifies that PCA should fit the data to find \n",
    "# the 30 principal components.\n",
    "pca = PCA(n_components=30, random_state=2020)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Store the values of the (30) principal components in the variable X_pca\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"X_pca:\\n\", X_pca)\n",
    "\n",
    "# Since we selected the full set of variables, the selected components \n",
    "# should account for 100% of the variance in the data.\n",
    "print(\"\\n => Variance explained by the components:\", sum(pca.explained_variance_ratio_ * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "0C-Vj6zTFOyn",
    "outputId": "41a9a994-eaee-4020-b8b7-270e015561fd"
   },
   "outputs": [],
   "source": [
    "# If we plot the variance as a function of the number of components, we can observe\n",
    "# the minimum number of components needed to explain a certain percentage of the variance.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained by PCA Components\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjfIN5w5GPf3",
    "outputId": "831380a8-9214-48b2-bec6-b4b0f5dd4125"
   },
   "outputs": [],
   "source": [
    "# We see that with just a third of the variables, we can explain 95% of the variance\n",
    "n_var = np.cumsum(pca.explained_variance_ratio_ * 100)[9]\n",
    "print(\"Variance explained by the first 10 components:\", n_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "_8JyvIdmJegD",
    "outputId": "48f1877a-a637-4dbf-93ad-4eeca88618c3"
   },
   "outputs": [],
   "source": [
    "# Alternatively, we can construct the subset that accommodates 95% of the variance\n",
    "# as follows\n",
    "pca_95 = PCA(n_components=0.95, random_state=2020)\n",
    "pca_95.fit(X_scaled)\n",
    "X_pca_95 = pca_95.transform(X_scaled)\n",
    "\n",
    "# A good practice is to visualize the relationship between the principal components\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=X_pca_95[:, 0], y=X_pca_95[:, 1], hue=cancer_data.target, palette=\"Set1\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Scatter Plot of First Two Principal Components\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDb7OqrhEg-_"
   },
   "source": [
    "## Description of the Principal Components Plot\n",
    "\n",
    "1. **Dimensionality Reduction**: PCA is a dimensionality reduction technique that transforms the original variables into a new set of variables (principal components) that are orthogonal (uncorrelated) to each other. These principal components capture the majority of the variability present in the original data.\n",
    "\n",
    "2. **Visualization of Principal Components**: By plotting the first two principal components (`X_pca_95[:, 0]` and `X_pca_95[:, 1]`), this graph shows the dispersion of the data in the two directions that capture the most variability. Each point in the graph represents an observation in this reduced component space.\n",
    "\n",
    "3. **Interpretation of Axes**: The axes of the graph (x-axis and y-axis) do not have inherent meaning in terms of the original variables since each principal component is a linear combination of them. However, the relative position of the points can indicate patterns and relationships among the samples.\n",
    "\n",
    "This type of graph is useful for visually exploring the structure of the data. For example:\n",
    "- If points from different categories are clearly separated, it suggests that the first two principal components are effective at distinguishing these categories.\n",
    "- Conversely, if there is significant overlap, it may indicate that more information (additional components or alternative techniques) is needed to effectively differentiate the classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bA7AHBYYLjCq",
    "outputId": "a3fbc1a2-9c56-4052-bfd5-5232bd58022d"
   },
   "outputs": [],
   "source": [
    "# Finally, we can create a new DataFrame with the result of the PCA analysis\n",
    "cols = ['PCA' + str(i) for i in range(10)]\n",
    "df_pca = pd.DataFrame(X_pca_95, columns=cols)\n",
    "print(\"Data (PCA - 95%):\\n\", df_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMNikQjlr2cR",
    "outputId": "8e2c7e9c-a3f3-4bd5-b6e8-ed215eb2e405"
   },
   "outputs": [],
   "source": [
    "# Obtain the component matrix\n",
    "components = pca.components_\n",
    "\n",
    "# Create a DataFrame with the component loadings\n",
    "df_loadings = pd.DataFrame(components.T, columns=['PC' + str(i + 1) for i in range(components.shape[0])], index=df.columns)\n",
    "\n",
    "# Display the loadings\n",
    "print(df_loadings)\n",
    "\n",
    "# For each principal component, find the original variable with the greatest influence\n",
    "for i in range(components.shape[0]):\n",
    "    pc = f'PC{i + 1}'\n",
    "    most_influential_variable = df_loadings[pc].abs().idxmax()\n",
    "    print(f\"{pc}: {most_influential_variable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51b2ifyOsyDs",
    "outputId": "56eefe71-5ba4-4b4b-89dd-70381e298fee"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the example dataset (Iris)\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Apply SelectKBest\n",
    "k = 3  # Select the top 3 features\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selected_feature_indices].tolist()\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "X_selected = pd.DataFrame(X_new, columns=selected_features)\n",
    "\n",
    "print(\"\\nData with Selected Features:\")\n",
    "print(X_selected.head())\n",
    "\n",
    "# Get the scores for all features\n",
    "scores = selector.scores_\n",
    "feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\n",
    "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"\\nScores for All Features:\")\n",
    "print(feature_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "n06V6SN1X_mu",
    "outputId": "33fcaf63-5e0c-4905-9135-df3ac30ab4e1"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(selector, feature_names, figsize=(10, 6), palette=\"viridis\"):\n",
    "    \"\"\"\n",
    "    Generates a bar chart showing the importance of features \n",
    "    based on the SelectKBest analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve scores and create a DataFrame\n",
    "    scores = selector.scores_\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': scores\n",
    "    })\n",
    "\n",
    "    # Sort by descending score\n",
    "    feature_scores = feature_scores.sort_values('Score', ascending=True)\n",
    "\n",
    "    # Create the figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Generate the horizontal bar chart\n",
    "    ax = sns.barplot(\n",
    "        data=feature_scores,\n",
    "        y='Feature',\n",
    "        x='Score',\n",
    "        palette=palette,\n",
    "        hue='Feature'\n",
    "    )\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Feature Importance According to SelectKBest', pad=20)\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "    # Add values to the bars\n",
    "    for i, v in enumerate(feature_scores['Score']):\n",
    "        ax.text(v, i, f'{v:.2f}', va='center')\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "fig = plot_feature_importance(selector, X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pe86_D2lWqb"
   },
   "source": [
    "## Independent Component Analysis (ICA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qP5KjpDTlSHz",
    "outputId": "f976769e-8f2f-41c9-d530-09bd295c50fc"
   },
   "outputs": [],
   "source": [
    "# We will use fMRI data for our example with ICA.\n",
    "# To do this, we start by installing the nilearn library.\n",
    "!python -m pip install nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD8hbq5mmD2s",
    "outputId": "a401aa1a-01b5-4571-dced-698ac2badea3"
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# Download an fMRI subject from the developmental study\n",
    "dataset = datasets.fetch_development_fmri(n_subjects=1)\n",
    "file_name = dataset.func[0]\n",
    "\n",
    "# Preprocessing the image\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# Apply a mask to extract the background from the image (non-brain voxels)\n",
    "masker = NiftiMasker(smoothing_fwhm=8, memory='nilearn_cache', memory_level=1,\n",
    "                     mask_strategy='epi', standardize=True)\n",
    "data_masked = masker.fit_transform(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs474WXDmZQZ",
    "outputId": "6faee020-a545-4c34-d0f5-83194b4c15f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "import numpy as np\n",
    "\n",
    "# Select 10 components\n",
    "ica = FastICA(n_components=10, random_state=42)\n",
    "components_masked = ica.fit_transform(data_masked.T).T\n",
    "\n",
    "# Apply a threshold (80% signal) to the data after normalization\n",
    "# based on mean and standard deviation\n",
    "components_masked -= components_masked.mean(axis=0)\n",
    "components_masked /= components_masked.std(axis=0)\n",
    "components_masked[np.abs(components_masked) < .8] = 0\n",
    "\n",
    "# Invert the transformation to recover the 3D structure\n",
    "component_img = masker.inverse_transform(components_masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "id": "y8u270f2nEm-",
    "outputId": "6c11a149-3831-4b89-df02-90ff68c7d648"
   },
   "outputs": [],
   "source": [
    "# Finally, we visualize the results of the dimensionality reduction operations\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "from nilearn import datasets\n",
    "\n",
    "# Data for a subject/patient\n",
    "dataset = datasets.fetch_development_fmri(n_subjects=1)\n",
    "func_filename = dataset.func[0]\n",
    "\n",
    "# Calculate the mean image\n",
    "mean_img = image.mean_img(func_filename)\n",
    "\n",
    "# Plot the first and second independent components over the mean image\n",
    "plot_stat_map(image.index_img(component_img, 0), mean_img)\n",
    "plot_stat_map(image.index_img(component_img, 1), mean_img)\n",
    "show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAkdGwgJw+gRWm42XKhmYa",
   "include_colab_link": true,
   "name": "2_3_3_Extraccion_de_caracteristicas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
