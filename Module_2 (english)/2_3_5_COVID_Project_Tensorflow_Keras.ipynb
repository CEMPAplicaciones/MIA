{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_5_Proyecto_COVID_Tensorflow_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWFHtS42bOUV"
   },
   "source": [
    "# COVID-19 Patient Classification\n",
    "\n",
    "In this guided project, we will implement a deep neural network for the classification or diagnosis of patients suspected of COVID-19 infection using medical imaging data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deumR1bMLe8f"
   },
   "source": [
    "## Modify Execution Environment and Select GPU Support\n",
    "\n",
    "Before starting to write code, we will modify the Google Colab execution environment to work with GPU hardware. To do this, go to the 'Runtime' menu, select the option 'Change runtime type,' and in the panel that opens, choose GPU from the 'Hardware accelerator' dropdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMRiBWpYAiOQ"
   },
   "source": [
    "# A) Dataset Import\n",
    "\n",
    "We load the medical images after downloading them from [this link](https://drive.google.com/file/d/1C6nEoNFr8PmqEddHHYGnXGAWNynz22qm/view?usp=sharing) in the provided .zip file. We will use the left-hand panel to make them available on the virtual machine that will execute this notebook. \n",
    "\n",
    "The Data folder contains over two thousand 2D chest X-ray images in jpg format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PEMxN9iWLQR"
   },
   "source": [
    "Options for Loading the Data:\n",
    "\n",
    "1. If you have a Google Drive account, mount the drive and load the Data folder from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1_ZGV8IVlYd",
    "outputId": "d78bb5b3-8db1-428d-ceb9-e973d545356d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# The base directory with the Data folder in Google Drive\n",
    "base_dir = '/content/drive/MyDrive/Documentos/Master IA/Datasets/Datos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VPnD-ifWmwR"
   },
   "source": [
    "2. Upload the .zip file and unzip it in the VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2wmbZj2U3gY",
    "outputId": "1a07cc7a-4069-4818-a7aa-3629b4360e37"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/Datos.zip\" -d \"/content/Datos/\"\n",
    "base_dir = '/content/datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWxzGS08AcVK"
   },
   "source": [
    "# B) Importing Necessary Libraries\n",
    "We load the Sequential library for configuring the network composed of convolutional layers, 2D max pooling layers, dropout layers, and flatten and dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lY1sIb7GI_m5",
    "outputId": "eee0911a-2e4d-462f-fcb4-86f09e0f1562"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # Base class for building the network layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "# Conv2D -> Used for edge detection and sharpening image definition\n",
    "# Pooling -> Reduces data size and image dimensionality\n",
    "# Dropout -> Controls model overfitting\n",
    "# Flatten -> Converts the feature matrix into a 1D vector\n",
    "# Dense -> Connects the feature vector with the input data vector, used for label predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Adam will be used as the optimizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Allows for data augmentation tasks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n - A) Imported necessary libraries\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv3ykJOtAn7k"
   },
   "outputs": [],
   "source": [
    "# Define the paths to the folders containing training and validation images\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# Base directory with 'train' and 'test' folders containing images \n",
    "# for training and validating the model\n",
    "base_dir = '/content/datasets/Data'\n",
    "train_dir = op.join(base_dir, 'train')\n",
    "test_dir = op.join(base_dir, 'test')\n",
    "\n",
    "# Within each set, there are two subdirectories with chest X-rays:\n",
    "# one for COVID and one for Normal cases\n",
    "train_covid_dir = op.join(train_dir, 'COVID19')\n",
    "train_normal_dir = op.join(train_dir, 'NORMAL')\n",
    "test_covid_dir = op.join(test_dir, 'COVID19')\n",
    "test_normal_dir = op.join(test_dir, 'NORMAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_QyIqfcBUwv",
    "outputId": "4b207975-95c3-41fc-bdec-31fd973a0a07"
   },
   "outputs": [],
   "source": [
    "# Preview the images\n",
    "# Training set\n",
    "train_covid_names = sorted(os.listdir(train_covid_dir))\n",
    "print(\"\\n - First 10 training images (COVID):\", train_covid_names[0:10])\n",
    "train_normal_names = sorted(os.listdir(train_normal_dir))\n",
    "print(\"\\n - First 10 training images (NORMAL):\", train_normal_names[0:10])\n",
    "\n",
    "# Validation set\n",
    "test_covid_names = sorted(os.listdir(test_covid_dir))\n",
    "print(\"\\n - First 10 validation images (COVID):\", test_covid_names[0:10])\n",
    "test_normal_names = sorted(os.listdir(test_normal_dir))\n",
    "print(\"\\n - First 10 validation images (NORMAL):\", test_normal_names[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_PDJQWuBU8U",
    "outputId": "dcae2f14-8019-4499-cb81-5417b4cc99d3"
   },
   "outputs": [],
   "source": [
    "# How many images are in our datasets\n",
    "print(\" => Images in the training set:\", len(train_covid_names) + len(train_normal_names))\n",
    "print(\" => Images in the validation set:\", len(test_covid_names) + len(test_normal_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMPZ9t8UBogq"
   },
   "source": [
    "# C) Viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "vryI0-PKBtLF",
    "outputId": "547af0f4-8cd7-4d3a-f3b7-8dcb365a2f74"
   },
   "outputs": [],
   "source": [
    "# Let's visualize some images from the dataset on a 4x4 grid\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(4, 4, figure=fig)\n",
    "covid_pics = [op.join(train_covid_dir, filename) for filename in train_covid_names[0:8]]\n",
    "normal_pics = [op.join(train_normal_dir, filename) for filename in train_normal_names[0:8]]\n",
    "merger_pics = covid_pics + normal_pics\n",
    "for i, pic_path in enumerate(merger_pics):\n",
    "  pic_name = op.basename(merger_pics[i])\n",
    "  ax = fig.add_subplot(gs[i])\n",
    "  pic_data = mpimg.imread(pic_path)\n",
    "  ax.imshow(pic_data, cmap='gray')\n",
    "  ax.set_title(pic_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWiQARLdCB9J"
   },
   "source": [
    "# D) Pre-processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZU7C28QWB_il",
    "outputId": "121ea3a2-49e7-4d44-be5d-b25c04c14f52"
   },
   "outputs": [],
   "source": [
    "# Generate training, evaluation, and validation batches\n",
    "dgen_train = ImageDataGenerator(rescale=1./255,\n",
    "                                validation_split=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)\n",
    "dgen_test = ImageDataGenerator(rescale=1./255)\n",
    "dgen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Apply rescaling for normalization (0-1 range) and data augmentation on the training set\n",
    "# zoom_range -> maximum percentage of zoom applied to the image\n",
    "# horizontal_flip -> apply horizontal flipping to the image\n",
    "\n",
    "# Create batch generators for training (80%) and validation (20%)\n",
    "train_generator = dgen_train.flow_from_directory(train_dir,\n",
    "                                                 target_size=(150, 150),\n",
    "                                                 subset='training',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "# Target size -> resize images to fit a 150x150 pixel frame\n",
    "# Mode -> specifies these data are for training\n",
    "# batch_size -> number of images loaded into memory per step\n",
    "# class_mode -> 'binary' for binary classification (COVID / Normal), 'categorical' for multi-class labels\n",
    "\n",
    "val_generator = dgen_train.flow_from_directory(train_dir,\n",
    "                                               target_size=(150, 150),\n",
    "                                               subset='validation',\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='binary')\n",
    "\n",
    "test_generator = dgen_train.flow_from_directory(test_dir,\n",
    "                                                target_size=(150, 150),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIdVxgoLGb0K",
    "outputId": "22a9c303-155a-44f1-99ba-ce674bae5654"
   },
   "outputs": [],
   "source": [
    "# As we know, the two classes of our problem are COVID and Normal, let's see it:\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMe4b6_YGdlt",
    "outputId": "d0926c92-4d1a-4acf-b766-8a2f61b3d04e"
   },
   "outputs": [],
   "source": [
    "# Let's check the sample size for model training\n",
    "train_generator.image_shape\n",
    "# The image size is 150x150, and the third dimension indicates that the images are in RGB format,\n",
    "# where the color of each pixel is a combination of red, green, and blue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELcsfIibGoK3"
   },
   "source": [
    "# E) Construction of the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-pwXOyuGxIq",
    "outputId": "5bfd4ecc-a247-4551-c055-ee656579c5c2"
   },
   "outputs": [],
   "source": [
    "# At this point, we can define our convolutional neural network (CNN) model\n",
    "# that will learn from the grouped data we processed earlier.\n",
    "# We will build the model by adding layers to an instance of the Sequential class.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer used to extract features from the image is a convolutional layer \n",
    "# that applies filters formed by small squares mapping the input image.\n",
    "# We will select 32 features to extract.\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu', input_shape=(150, 150, 3))) \n",
    "# Add the Pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Add a Dropout layer to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a second convolutional layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu'))\n",
    "# Add subsequent Pooling and Dropout layers\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Now, add the Flatten layer to transform the tensor into a 1D vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a densely connected (Dense) layer specifying the number of nodes and the activation function\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Add another Dropout operation to halve the number of nodes\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Finally, connect the nodes to create the output with a single node.\n",
    "# Since this is a classification problem, we use the sigmoid activation function.\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Model summary\n",
    "print(\"Model Summary:\\n\", model.summary())\n",
    "# In this summary, the size of the first tensor has the value None because it refers\n",
    "# to the batch size dimension, which can take any value for flexibility.\n",
    "\n",
    "# After applying the convolutional layer, we get a tensor with dimensions equal to \n",
    "# half the size of the input image and with 32 features in the z-axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgCag-_HHG2N"
   },
   "source": [
    "# F) Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iVcxB1ZHNDV",
    "outputId": "aa5d24af-afd0-4784-d6a4-b6e548a43e36"
   },
   "outputs": [],
   "source": [
    "# To compile our CNN model we need to:\n",
    "# 1) Define the optimization method (Adam)\n",
    "# 2) Set the learning rate value\n",
    "# 3) Choose the loss function: binary cross-entropy is a good choice for binary classification tasks\n",
    "# 4) Set the evaluation metric: we will use accuracy\n",
    "model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWtZjCFlHPQs",
    "outputId": "a4ca67f0-8150-4cf8-ca62-805a7d4b077c"
   },
   "outputs": [],
   "source": [
    "# Once the model is compiled, we can start the training process\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=30,\n",
    "                    validation_data=val_generator)\n",
    "# The history object records the progress during training, capturing the loss function value\n",
    "# and the evaluation metric at each step of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z95l8IC5HRoG"
   },
   "source": [
    "# G) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0d8G-BEHXXs",
    "outputId": "e74135ab-edff-4d92-b283-8fcc39d53152"
   },
   "outputs": [],
   "source": [
    "# Let's see that the values of loss and accuracy have been stored in history\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "EVtqI0QuHbZR",
    "outputId": "f4d63060-5472-4bd9-9ec3-d83a11a340ac"
   },
   "outputs": [],
   "source": [
    "# To evaluate the model's performance, we plot the values of the metrics of interest as a function of the\n",
    "# training epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss (training)', 'loss (validation)'])\n",
    "plt.title(\"Evolution of the loss function value\")\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "CEIibDQOHgfT",
    "outputId": "be761c5c-b943-486f-e1a9-d935ccf7bbd5"
   },
   "outputs": [],
   "source": [
    "# A look at the evolution of accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['Accuracy (training)', 'Accuracy (validation)'])\n",
    "plt.title(\"Evolution of accuracy value\")\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9Zm__wJHm3v",
    "outputId": "82524dc7-baa2-4d11-c2dd-2779434a1e07"
   },
   "outputs": [],
   "source": [
    "# Finally, we validate the model on the evaluation sample\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "# We simply need to evaluate the model we just trained on the dataset that we had\n",
    "# reserved for evaluation\n",
    "print(\"\\n=> Results on the test set:\")\n",
    "print(\" - Loss: {:.2f}, Accuracy: {:.2f} \".format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AZjx1Q2HseZ"
   },
   "source": [
    "# H) Results (predictions) on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "b5qflf3jH3o_",
    "outputId": "f07ad29c-5f8e-45e2-add9-ca9bb2a38240"
   },
   "outputs": [],
   "source": [
    "# In this last step, we will evaluate our model on the entire set of chest X-ray images\n",
    "# to obtain a result (prediction) for the patient: COVID infection or healthy\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "uploads = files.upload()\n",
    "for filename in uploads.keys():\n",
    "  img_path = '/content/' + filename\n",
    "  img = image.load_img(img_path, target_size=(150, 150))\n",
    "  data = image.img_to_array(img)\n",
    "  data = np.expand_dims(data, axis=0)\n",
    "  prediction = model.predict(data)\n",
    "  print(\"\\nX-ray Image:\", filename)\n",
    "\n",
    "  if prediction == 0:\n",
    "    print(\" => COVID-19 Detected\")\n",
    "  else:\n",
    "    print(\" => Normal Status\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CWFHtS42bOUV",
    "vWxzGS08AcVK",
    "hMRiBWpYAiOQ",
    "NMPZ9t8UBogq",
    "UWiQARLdCB9J"
   ],
   "include_colab_link": true,
   "name": "2_3_5_Proyecto_COVID_Tensorflow_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
